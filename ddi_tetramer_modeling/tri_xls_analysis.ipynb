{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08d81fd",
   "metadata": {},
   "source": [
    "***Analysis script for the trifunctional crosslinks of DDI1_DDI2 tetramer protein complex***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "012fa2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d5475a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw trifunctional XL-MS data: \n",
      "    Protein1 Residue1 Protein2 Residue2 Protein3 Residue3\n",
      "0      DDI1       77     DDI1      161     DDI1      291\n",
      "1      DDI1       77     DDI1      291     DDI1      382\n",
      "2      DDI1      133     DDI1      133     DDI1      213\n",
      "3      DDI1      133     DDI1      133     DDI1      291\n",
      "4      DDI1      133     DDI1      161     DDI1      213\n",
      "5      DDI1      133     DDI1      161     DDI1      291\n",
      "6      DDI1      133     DDI1      161     DDI1      345\n",
      "7      DDI1      133     DDI1      161     DDI2      337\n",
      "8      DDI1      133     DDI1      213     DDI1      291\n",
      "9      DDI1      133     DDI1      213     DDI1      345\n",
      "10     DDI1      133     DDI1      213     DDI2      337\n",
      "11     DDI1      133     DDI1      291     DDI1      291\n",
      "12     DDI1      133     DDI1      291     DDI1      382\n",
      "13     DDI1      161     DDI1      161     DDI1      291\n",
      "14     DDI1      161     DDI1      213     DDI1      291\n",
      "15     DDI1      161     DDI1      291     DDI1      382\n",
      "16     DDI1      190     DDI1      291     DDI1      382\n",
      "17     DDI1       77     DDI1      213     DDI1      291\n",
      "18     DDI1      213     DDI1      291     DDI1      382\n",
      "19     DDI1      291     DDI1      291     DDI1      345\n",
      "20     DDI1      291     DDI1      291     DDI1      382\n",
      "non-redundant trifunctional XL-MS data: \n",
      "    Protein1 Residue1 Protein2 Residue2 Protein3 Residue3\n",
      "0      DDI1       77     DDI1      161     DDI1      291\n",
      "1      DDI1       77     DDI1      291     DDI1      382\n",
      "2      DDI1      133     DDI1      133     DDI1      213\n",
      "3      DDI1      133     DDI1      133     DDI1      291\n",
      "4      DDI1      133     DDI1      161     DDI1      213\n",
      "5      DDI1      133     DDI1      161     DDI1      291\n",
      "6      DDI1      133     DDI1      161     DDI1      345\n",
      "7      DDI1      133     DDI1      161     DDI2      337\n",
      "8      DDI1      133     DDI1      213     DDI1      291\n",
      "9      DDI1      133     DDI1      213     DDI1      345\n",
      "10     DDI1      133     DDI1      213     DDI2      337\n",
      "11     DDI1      133     DDI1      291     DDI1      291\n",
      "12     DDI1      133     DDI1      291     DDI1      382\n",
      "13     DDI1      161     DDI1      161     DDI1      291\n",
      "14     DDI1      161     DDI1      213     DDI1      291\n",
      "15     DDI1      161     DDI1      291     DDI1      382\n",
      "16     DDI1      190     DDI1      291     DDI1      382\n",
      "17     DDI1       77     DDI1      213     DDI1      291\n",
      "18     DDI1      213     DDI1      291     DDI1      382\n",
      "19     DDI1      291     DDI1      291     DDI1      345\n",
      "20     DDI1      291     DDI1      291     DDI1      382\n",
      "Residue 346 is a lysine in DDI1\n",
      "Residue 337 is a lysine in DDI2\n"
     ]
    }
   ],
   "source": [
    "# Read in the csv file for the trifunctional XL-MS data\n",
    "# path to current working directory \n",
    "dir_path = os.getcwd()\n",
    "xls_data_dir = os.path.join(dir_path, 'derived_data/xls/')\n",
    "fasta_data_dir = os.path.join(dir_path, 'data/fasta/')\n",
    "df = pd.read_csv(os.path.join(xls_data_dir, 'ddi_trifunctional.csv'))\n",
    "\n",
    "# Use some biopython module to read in fasta files for analysis later\n",
    "# Read in fasta files for DDI1 and DDI2\n",
    "ddi1_fasta = list(SeqIO.parse(os.path.join(fasta_data_dir, 'ddi1.fasta'), \"fasta\"))\n",
    "ddi2_fasta = list(SeqIO.parse(os.path.join(fasta_data_dir, 'ddi2.fasta'), \"fasta\"))\n",
    "\n",
    "# Remove the K from residue1/2/3 columns \n",
    "df['Residue1'] = df['Residue1'].str.replace('K', '')\n",
    "df['Residue2'] = df['Residue2'].str.replace('K', '')\n",
    "df['Residue3'] = df['Residue3'].str.replace('K', '')\n",
    "print(f\"raw trifunctional XL-MS data: \\n\", df)\n",
    "\n",
    "# Remove redundant rows, where the order of the residues in a row, as long as the protein \n",
    "# names are the same, does not matter\n",
    "df['residue_list'] = df[['Residue1', 'Residue2', 'Residue3']].values.tolist()\n",
    "df['residue_list'] = df['residue_list'].apply(lambda x: sorted(x))\n",
    "df['protein_list'] = df[['Protein1', 'Protein2', 'Protein3']].values.tolist()\n",
    "df['protein_list'] = df['protein_list'].apply(lambda x: sorted(x))\n",
    "df['unique_id'] = df['residue_list'].astype(str) + '_' + df['protein_list'].astype(str)\n",
    "df = df.drop_duplicates(subset=['unique_id'])\n",
    "df = df.drop(columns=['residue_list', 'protein_list', 'unique_id'])\n",
    "print(f\"non-redundant trifunctional XL-MS data: \\n\", df)\n",
    "\n",
    "# Function to check if residues are lysines in the given fasta record\n",
    "def check_lysines(fasta_record, residue_list):\n",
    "    sequence = str(fasta_record.seq)\n",
    "    lysine_positions = [m.start() + 1 for m in re.finditer('K', sequence)]  # +1 for 1-based indexing\n",
    "    for residue in residue_list:\n",
    "        if int(residue) not in lysine_positions:\n",
    "            print(f\"Warning: Residue {residue} is not a lysine in {fasta_record.id}\")\n",
    "        else:\n",
    "            print(f\"Residue {residue} is a lysine in {fasta_record.id}\")\n",
    "\n",
    "# Check residues in DDI1\n",
    "check_lysines(ddi1_fasta[0], [346])\n",
    "check_lysines(ddi2_fasta[0], [337]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b489b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: DSSP analysis failed for data/pdb/ddi1.pdb: [Errno 2] No such file or directory: '/Users/sreeganeshbalasubramani/.local/bin/mkdssp'\n",
      "Domain analysis completed for DDI1\n",
      "Found 3 domains: [(1, 79), (142, 290), (295, 371)]\n",
      "Results saved to: output_data/domain_analysis/ddi1.json\n",
      "Warning: DSSP analysis failed for data/pdb/ddi2.pdb: [Errno 2] No such file or directory: '/Users/sreeganeshbalasubramani/.local/bin/mkdssp'\n",
      "Domain analysis completed for DDI2\n",
      "Found 3 domains: [(1, 78), (133, 363), (386, 398)]\n",
      "Results saved to: output_data/domain_analysis/ddi2.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB import PDBParser, DSSP\n",
    "\n",
    "class DomainAnalyzer:\n",
    "    \"\"\"\n",
    "    A class to analyze protein domains using AlphaFold structure predictions\n",
    "    and DSSP secondary structure analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dssp_bin=\"dssp\"):\n",
    "        \"\"\"\n",
    "        Initialize the DomainAnalyzer.\n",
    "        \n",
    "        Args:\n",
    "            dssp_bin (str): Path to DSSP binary executable\n",
    "        \"\"\"\n",
    "        self.dssp_bin = dssp_bin\n",
    "        self.output_dir = Path(\"output_data/domain_analysis\")\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def get_disorder_from_alphafold(self, pdb_file):\n",
    "        \"\"\"\n",
    "        Use B-factor pLDDT<70 as disorder indicator per CA atom.\n",
    "        \n",
    "        Args:\n",
    "            pdb_file (str): Path to PDB file\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Boolean array indicating disorder for each residue\n",
    "        \"\"\"\n",
    "        disorder = []\n",
    "        with open(pdb_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"ATOM\") and line[12:16].strip() == \"CA\":\n",
    "                    b_factor = float(line[60:66])\n",
    "                    disorder.append(b_factor < 70)\n",
    "        return np.array(disorder)\n",
    "    \n",
    "    def find_domains(self, disorder_pred):\n",
    "        \"\"\"\n",
    "        Call contiguous ordered runs â‰¥8 aa as domains, merge small gaps.\n",
    "        \n",
    "        Args:\n",
    "            disorder_pred (np.array): Boolean array of disorder predictions\n",
    "            \n",
    "        Returns:\n",
    "            list: List of domain tuples (start, end) in 1-based indexing\n",
    "        \"\"\"\n",
    "        domains = []\n",
    "        start = None\n",
    "        n = len(disorder_pred)\n",
    "        \n",
    "        for i in range(n):\n",
    "            if not disorder_pred[i] and start is None:\n",
    "                start = i\n",
    "            if (disorder_pred[i] or i == n-1) and start is not None:\n",
    "                if i - start >= 8:\n",
    "                    domains.append((start + 1, i))  # 1-based indexing\n",
    "                start = None\n",
    "        \n",
    "        # Merge domains separated by <5 aa\n",
    "        merged = []\n",
    "        for dom in domains:\n",
    "            if not merged:\n",
    "                merged.append(dom)\n",
    "            else:\n",
    "                prev = merged[-1]\n",
    "                if dom[0] - prev[1] < 5:\n",
    "                    merged[-1] = (prev[0], dom[1])\n",
    "                else:\n",
    "                    merged.append(dom)\n",
    "        \n",
    "        return merged\n",
    "    \n",
    "    def run_dssp_analysis(self, pdb_file):\n",
    "        \"\"\"\n",
    "        Run DSSP to extract secondary structure per residue.\n",
    "        \n",
    "        Args:\n",
    "            pdb_file (str): Path to PDB file\n",
    "            \n",
    "        Returns:\n",
    "            DSSP object containing secondary structure information\n",
    "        \"\"\"\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        struct = parser.get_structure(\"protein\", pdb_file)\n",
    "        dssp = DSSP(struct[0], pdb_file, dssp=self.dssp_bin)\n",
    "        return dssp\n",
    "    \n",
    "    def extend_domains_with_ss(self, domains, pdb_file):\n",
    "        \"\"\"\n",
    "        Grow domain boundaries while adjacent residues are helix/strand.\n",
    "        \n",
    "        Args:\n",
    "            domains (list): List of domain tuples\n",
    "            pdb_file (str): Path to PDB file\n",
    "            \n",
    "        Returns:\n",
    "            list: List of extended domain tuples\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dssp_data = self.run_dssp_analysis(pdb_file)\n",
    "            ss = [residue[2] for residue in dssp_data]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: DSSP analysis failed for {pdb_file}: {e}\")\n",
    "            return domains\n",
    "        \n",
    "        extended = []\n",
    "        for start, end in domains:\n",
    "            i0, i1 = start - 1, end - 1  # Convert to 0-based\n",
    "            \n",
    "            # Extend start while previous residues are helix/strand\n",
    "            while i0 > 0 and ss[i0 - 1] in (\"H\", \"E\"):\n",
    "                i0 -= 1\n",
    "            \n",
    "            # Extend end while next residues are helix/strand\n",
    "            while i1 < len(ss) - 1 and ss[i1 + 1] in (\"H\", \"E\"):\n",
    "                i1 += 1\n",
    "            \n",
    "            extended.append((i0 + 1, i1 + 1))  # Convert back to 1-based\n",
    "        \n",
    "        return extended\n",
    "    \n",
    "    def get_protein_info(self, fasta_file):\n",
    "        \"\"\"\n",
    "        Extract protein information from FASTA file.\n",
    "        \n",
    "        Args:\n",
    "            fasta_file (str): Path to FASTA file\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing protein ID, description, and sequence length\n",
    "        \"\"\"\n",
    "        fasta_records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "        if not fasta_records:\n",
    "            raise ValueError(f\"No sequences found in {fasta_file}\")\n",
    "        \n",
    "        record = fasta_records[0]  # Take first sequence\n",
    "        return {\n",
    "            \"protein_id\": record.id,\n",
    "            \"description\": record.description,\n",
    "            \"sequence_length\": len(record.seq)\n",
    "        }\n",
    "    \n",
    "    def analyze_domains(self, fasta_file, pdb_file, output_name=None):\n",
    "        \"\"\"\n",
    "        Perform complete domain analysis and save results.\n",
    "        \n",
    "        Args:\n",
    "            fasta_file (str): Path to FASTA file\n",
    "            pdb_file (str): Path to PDB file\n",
    "            output_name (str, optional): Custom output filename (without extension)\n",
    "            \n",
    "        Returns:\n",
    "            dict: Domain analysis results\n",
    "        \"\"\"\n",
    "        # Get protein information\n",
    "        protein_info = self.get_protein_info(fasta_file)\n",
    "        \n",
    "        # Generate output filename if not provided\n",
    "        if output_name is None:\n",
    "            protein_id = protein_info[\"protein_id\"].replace(\"|\", \"_\")\n",
    "            output_name = f\"{protein_id}_domains\"\n",
    "        \n",
    "        # Perform domain analysis\n",
    "        disorder = self.get_disorder_from_alphafold(pdb_file)\n",
    "        initial_domains = self.find_domains(disorder)\n",
    "        extended_domains = self.extend_domains_with_ss(initial_domains, pdb_file)\n",
    "        \n",
    "        # Prepare results\n",
    "        results = {\n",
    "            \"protein_info\": protein_info,\n",
    "            \"analysis_parameters\": {\n",
    "                \"disorder_threshold\": 60,\n",
    "                \"minimum_domain_length\": 6,\n",
    "                \"merge_gap_threshold\": 4\n",
    "            },\n",
    "            \"input_files\": {\n",
    "                \"fasta_file\": str(fasta_file),\n",
    "                \"pdb_file\": str(pdb_file)\n",
    "            },\n",
    "            \"domain_analysis\": {\n",
    "                \"initial_domains\": initial_domains,\n",
    "                \"extended_domains\": extended_domains,\n",
    "                \"num_domains\": len(extended_domains),\n",
    "                \"total_domain_residues\": sum(end - start + 1 for start, end in extended_domains)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save to JSON file\n",
    "        output_file = self.output_dir / f\"{output_name}.json\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(f\"Domain analysis completed for {protein_info['protein_id']}\")\n",
    "        print(f\"Found {len(extended_domains)} domains: {extended_domains}\")\n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example usage function\n",
    "def analyze_ddi_proteins():\n",
    "    \"\"\"Example function to analyze DDI1 and DDI2 proteins.\"\"\"\n",
    "    mkdssp_path = os.path.expanduser(\"~/.local/bin/mkdssp\")\n",
    "    analyzer = DomainAnalyzer(dssp_bin=mkdssp_path)\n",
    "    \n",
    "    # Define file paths (adjust these to your actual file locations)\n",
    "    proteins = [\n",
    "        {\n",
    "            \"name\": \"DDI1\",\n",
    "            \"fasta\": \"data/fasta/ddi1.fasta\",\n",
    "            \"pdb\": \"data/pdb/ddi1.pdb\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DDI2\", \n",
    "            \"fasta\": \"data/fasta/ddi2.fasta\",\n",
    "            \"pdb\": \"data/pdb/ddi2.pdb\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    for protein in proteins:\n",
    "        if os.path.exists(protein[\"fasta\"]) and os.path.exists(protein[\"pdb\"]):\n",
    "            result = analyzer.analyze_domains(\n",
    "                protein[\"fasta\"], \n",
    "                protein[\"pdb\"], \n",
    "                protein[\"name\"].lower()\n",
    "            )\n",
    "            results[protein[\"name\"]] = result\n",
    "        else:\n",
    "            print(f\"Warning: Files not found for {protein['name']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run analysis\n",
    "    results = analyze_ddi_proteins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01746063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data dimensions: (118, 23)\n",
      "bifunctional data dimensions: (99, 14)\n",
      "rows with multiple entries dimensions: (14, 14)\n",
      "bifunctional data dimensions after removing multi-entry rows: (85, 14)\n"
     ]
    }
   ],
   "source": [
    "# Analysis of the original dataset to obtain csv file for the bifunctional crosslinks \n",
    "\n",
    "# Read in the csv file containing the raw data\n",
    "rawdata_file = os.path.join(xls_data_dir, 'raw_data.csv')\n",
    "# make sure that empty values are read as empty strings and not NaN\n",
    "df_raw = pd.read_csv(rawdata_file, dtype=str)\n",
    "print(f\"raw data dimensions: {df_raw.shape}\")\n",
    "# Process only bifunctional crosslinks from this raw data, if the entry in the column \n",
    "# Gene C is empty, it is a bifunctional crosslink, in this case, drop the\n",
    "# columns Gene C, Resi C, and any other columns that have empty values for that row, and also\n",
    "# drop rows that have non empty values for Gene C and Resi C\n",
    "df_bifunctional = df_raw[df_raw['Gene C'].isna()].copy()\n",
    "df_bifunctional = df_bifunctional.drop(columns=['Gene C', 'Resi C', 'Peptide', 'R1_L_only', 'R2_L_only', 'R1_H_only', 'R2_H_only', 'R1_Mixed', 'R2_Mixed'])\n",
    "# Drop any other columns that have empty values for that row\n",
    "df_bifunctional = df_bifunctional.dropna(axis=1, how='all')\n",
    "print(f\"bifunctional data dimensions: {df_bifunctional.shape}\")\n",
    "# Remove duplicate rows\n",
    "#df_bifunctional = df_bifunctional.drop_duplicates() \n",
    "#print(f\"bifunctional data dimensions after removing duplicates: {df_bifunctional.shape}\")\n",
    "# rename column names Gene A to Protein1, Resi A to Residue1, Gene B to Protein2, Resi B to Residue2\n",
    "df_bifunctional = df_bifunctional.rename(columns={'Gene A': 'Protein1', 'Resi A': 'Residue1', \n",
    "                                                  'Gene B': 'Protein2', 'Resi B': 'Residue2'})\n",
    "\n",
    "# Some rows have entries such as DDI1, DDI2 in the Gene A/B columns \n",
    "# and also entries correspondingly in the Resi A/B columns with a comma separating \n",
    "# two residue numbers, in this case we need to create two separate rows, for example if the \n",
    "# entries are DDI1, DDI2 in Gene A/B and 345, 337 in Resi A/B, we need to create two rows\n",
    "# row 1 will have DDI1 in whichever column it was present in and 345 in the the corresponding\n",
    "# Resi column, and row 2 will have DDI2 in the Gene column and 337 in the Resi column\n",
    "# use regex to check if there is a comma or ; in any of the entries\n",
    "#print(df_bifunctional)\n",
    "pattern = re.compile(r'[;,]')\n",
    "# only copy rows with these patterns separately, and we shall process them now\n",
    "pattern = re.compile(r'[;,]')\n",
    "df_multi = df_bifunctional[df_bifunctional.apply(\n",
    "    lambda row: (bool(pattern.search(str(row['Protein1']))) or \n",
    "                 bool(pattern.search(str(row['Protein2']))) or \n",
    "                 bool(pattern.search(str(row['Residue1']))) or \n",
    "                 bool(pattern.search(str(row['Residue2'])))), axis=1)].copy()\n",
    "print(f\"rows with multiple entries dimensions: {df_multi.shape}\")\n",
    "# remove the df_multi from df_bifunctional\n",
    "df_bifunctional = df_bifunctional.drop(df_multi.index)\n",
    "print(f\"bifunctional data dimensions after removing multi-entry rows: {df_bifunctional.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d09c091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploded DataFrame:\n",
      "exploded data dimensions: (68, 14)\n",
      "final bifunctional data dimensions: (153, 14)\n",
      "bifunctional data dimensions after removing duplicates: (74, 15)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from itertools import product\n",
    "\n",
    "def split_cell(cell):\n",
    "    \"\"\"\n",
    "    Splits a cell on commas or semicolons.\n",
    "    Strips whitespace and ignores empty strings.\n",
    "    Returns a list of entries.\n",
    "    \"\"\"\n",
    "    if pd.isna(cell):\n",
    "        return []\n",
    "    parts = re.split(r'[;,]', str(cell))\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def split_row(row):\n",
    "    \"\"\"\n",
    "    Given a row, split the multi-entry key columns (Protein1/Residue1 and Protein2/Residue2)\n",
    "    by generating the full Cartesian product.\n",
    "    Non-key columns are copied unchanged.\n",
    "    \"\"\"\n",
    "    new_rows = []\n",
    "    \n",
    "    # Split key columns\n",
    "    p1_list = split_cell(row['Protein1'])\n",
    "    r1_list = split_cell(row['Residue1'])\n",
    "    p2_list = split_cell(row['Protein2'])\n",
    "    r2_list = split_cell(row['Residue2'])\n",
    "    \n",
    "    # If splitting returned an empty list, revert to original string.\n",
    "    if not p1_list: p1_list = [row['Protein1']]\n",
    "    if not r1_list: r1_list = [row['Residue1']]\n",
    "    if not p2_list: p2_list = [row['Protein2']]\n",
    "    if not r2_list: r2_list = [row['Residue2']]\n",
    "    \n",
    "    # If one column in a pair is single and the other multiple, replicate the single value.\n",
    "    if len(p1_list) == 1 and len(r1_list) > 1:\n",
    "        p1_list = p1_list * len(r1_list)\n",
    "    elif len(r1_list) == 1 and len(p1_list) > 1:\n",
    "        r1_list = r1_list * len(p1_list)\n",
    "        \n",
    "    if len(p2_list) == 1 and len(r2_list) > 1:\n",
    "        p2_list = p2_list * len(r2_list)\n",
    "    elif len(r2_list) == 1 and len(p2_list) > 1:\n",
    "        r2_list = r2_list * len(p2_list)\n",
    "    \n",
    "    # Take the cross product between the two pairs (i.e. full Cartesian product).\n",
    "    for (p1, r1) in product(p1_list, r1_list):\n",
    "        for (p2, r2) in product(p2_list, r2_list):\n",
    "            new_row = row.copy()\n",
    "            new_row['Protein1'] = p1\n",
    "            new_row['Residue1'] = r1\n",
    "            new_row['Protein2'] = p2\n",
    "            new_row['Residue2'] = r2\n",
    "            new_rows.append(new_row)\n",
    "    \n",
    "    return new_rows\n",
    "\n",
    "def explode_multiple_entries(df):\n",
    "    \"\"\"\n",
    "    Expand the DataFrame by exploding rows that contain multiple entries in the key columns.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        exploded = split_row(row)\n",
    "        rows.extend(exploded)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Explode the DataFrame:\n",
    "df_exploded = explode_multiple_entries(df_multi)\n",
    "print(\"Exploded DataFrame:\")\n",
    "#print(df_exploded)\n",
    "print(f\"exploded data dimensions: {df_exploded.shape}\")\n",
    "# merge df_exploded into df_bifunctional - df_multi \n",
    "df_bifunctional = pd.concat([df_bifunctional, df_exploded], ignore_index=True)\n",
    "#print(\"Final Bifunctional DataFrame:\")\n",
    "#print(df_bifunctional)\n",
    "print(f\"final bifunctional data dimensions: {df_bifunctional.shape}\")\n",
    "\n",
    "# Process rows and compare them, for example, if the pair DDI1-345 and DDI2-337 \n",
    "# is repeated in two rows, then one of them need to be dropped, regardless of the order\n",
    "# of the entries in the row, i.e. DDI1-345 and DDI2-337 is the same as DDI2-337 and DDI1-345\n",
    "# Create a unique key by sorting the pairs (ignoring order)\n",
    "# First, create the 'pair_key' as before.\n",
    "df_bifunctional['pair_key'] = df_bifunctional.apply(\n",
    "    lambda row: '_'.join(sorted([f\"{row['Protein1']}-{row['Residue1']}\",\n",
    "                                  f\"{row['Protein2']}-{row['Residue2']}\"])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate frequency for each pair_key.\n",
    "frequency = df_bifunctional['pair_key'].value_counts()\n",
    "\n",
    "# Map the frequency to each row.\n",
    "df_bifunctional['frequency'] = df_bifunctional['pair_key'].map(frequency)\n",
    "\n",
    "# Drop duplicate rows based on pair_key and then remove pair_key.\n",
    "df_bifunctional_unique = df_bifunctional.drop_duplicates(subset=['pair_key']).drop(columns=['pair_key'])\n",
    "\n",
    "print(f\"bifunctional data dimensions after removing duplicates: {df_bifunctional_unique.shape}\")\n",
    "# Save to CSV\n",
    "bifunctional_file = os.path.join(xls_data_dir, 'ddi_bifunctional_unique_processed.csv')\n",
    "df_bifunctional_unique.to_csv(bifunctional_file, index=False)\n",
    "\n",
    "bifunctional_file = os.path.join(xls_data_dir, 'ddi_bifunctional_processed.csv')\n",
    "df_bifunctional.to_csv(bifunctional_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec2be1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered bifunctional data dimensions: (17, 15)\n",
      "   Protein1 Residue1 Protein2 Residue2 NR1_L_only NR1_Mixed NR1_H_only  \\\n",
      "0      DDI1       31     DDI1       31       0.81      0.51          1   \n",
      "1      DDI1       31     DDI1       77          1      0.07       0.57   \n",
      "2      DDI1       31     DDI1      133          1      0.15       0.57   \n",
      "3      DDI1       31     DDI1      161          1       0.1       0.48   \n",
      "4      DDI1       31     DDI1      213          1      0.12       0.45   \n",
      "5      DDI2       67     DDI2       77          1      0.51       0.81   \n",
      "6      DDI2       77     DDI2      153          1      0.07       0.75   \n",
      "7      DDI1       77     DDI1      161          1      0.29       0.62   \n",
      "8      DDI1       77     DDI1      213          1      0.09       0.67   \n",
      "9      DDI1      133     DDI1      133          1      0.51       0.54   \n",
      "10     DDI1      133     DDI1      161          1      0.23       0.57   \n",
      "11     DDI1      133     DDI1      190          1      0.03       0.67   \n",
      "12     DDI1      133     DDI1      213          1      0.11       0.61   \n",
      "13     DDI1      161     DDI1      161          1      0.41       0.45   \n",
      "14     DDI1      161     DDI1      213          1      0.37       0.83   \n",
      "15     DDI1      213     DDI2       77          1      0.43       0.64   \n",
      "16     DDI1      213     DDI1      213       0.48      0.34          1   \n",
      "\n",
      "   NR2_L_only NR2_Mixed NR2_H_only R1_TSTO R2_TSTO Avg_TSTO  DSSO  frequency  \n",
      "0           1      0.54       0.76    0.56    0.61     0.59   NaN          1  \n",
      "1           1      0.26        0.9    0.08    0.27     0.18  0.01          1  \n",
      "2           1      0.15       0.69    0.19    0.18     0.18  0.15          1  \n",
      "3           1      0.11       0.44    0.13    0.15     0.14   NaN          1  \n",
      "4           1      0.21        0.5    0.17    0.28     0.22   NaN          1  \n",
      "5           1      0.78       0.89    0.56    0.82     0.69   NaN          1  \n",
      "6           1      0.11        0.5    0.08    0.15     0.12   NaN          1  \n",
      "7           1      0.21       0.56    0.36    0.26     0.31   NaN          2  \n",
      "8           1      0.17       0.96     0.1    0.18     0.14  0.13          2  \n",
      "9           1      0.34       0.31    0.67    0.52     0.59   0.8          3  \n",
      "10          1      0.22       0.54    0.29    0.28     0.29  0.35          4  \n",
      "11          1      0.03       0.69    0.04    0.04     0.04   NaN          1  \n",
      "12          1      0.09       0.55    0.14    0.12     0.13  0.26          5  \n",
      "13          1      0.65       0.59    0.56    0.82     0.69  0.96          2  \n",
      "14          1       0.3       0.67    0.41    0.36     0.38  0.41          3  \n",
      "15          1       0.3       0.52    0.53    0.39     0.46   NaN          1  \n",
      "16       0.69      0.45          1    0.46    0.54      0.5   NaN          1  \n"
     ]
    }
   ],
   "source": [
    "# Now, we look for residue numbers from the dataframe that are less than 234 for DDI1 and less than \n",
    "# 230 for DDI2 as these are the NTD domains, and we only use these crosslinks for modeling\n",
    "def filter_crosslinks(df):\n",
    "    cond1 = ((df['Protein1'] == 'DDI1') & (df['Residue1'].astype(int) < 234)) | \\\n",
    "            ((df['Protein1'] == 'DDI2') & (df['Residue1'].astype(int) < 230))\n",
    "    cond2 = ((df['Protein2'] == 'DDI1') & (df['Residue2'].astype(int) < 234)) | \\\n",
    "            ((df['Protein2'] == 'DDI2') & (df['Residue2'].astype(int) < 230))\n",
    "    return df[cond1 & cond2]\n",
    "\n",
    "df_filtered = filter_crosslinks(df_bifunctional_unique)\n",
    "# arrange the df_filtered in ascending order based on Residue1, so all rows will be sorted\n",
    "# based on this criteria\n",
    "df_filtered = df_filtered.sort_values(\n",
    "    by=['Residue1', 'Residue2'],\n",
    "    key=lambda col: col.astype(int)\n",
    ").reset_index(drop=True)\n",
    "print(f\"filtered bifunctional data dimensions: {df_filtered.shape}\")\n",
    "print(df_filtered)\n",
    "\n",
    "# write to file\n",
    "filtered_file = os.path.join(xls_data_dir, 'ddi_bifunctional_ntdomain_filtered.csv')\n",
    "df_filtered.to_csv(filtered_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52c9d4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimensions for IMP processing: (17, 4)\n",
      "   Protein1  Residue1 Protein2  Residue2  Copy1  Copy2  Iid\n",
      "0      DDI1        31     DDI1        31      0      1    1\n",
      "1      DDI1        31     DDI1        77      0      1    2\n",
      "2      DDI1        31     DDI1        77      1      0    2\n",
      "3      DDI1        31     DDI1       133      0      1    3\n",
      "4      DDI1        31     DDI1       133      1      0    3\n",
      "5      DDI1        31     DDI1       161      0      1    4\n",
      "6      DDI1        31     DDI1       161      1      0    4\n",
      "7      DDI1        31     DDI1       213      0      1    5\n",
      "8      DDI1        31     DDI1       213      1      0    5\n",
      "9      DDI1        77     DDI1       161      0      1    6\n",
      "10     DDI1        77     DDI1       161      1      0    6\n",
      "11     DDI1        77     DDI1       213      0      1    7\n",
      "12     DDI1        77     DDI1       213      1      0    7\n",
      "13     DDI1       133     DDI1       133      0      1    8\n",
      "14     DDI1       133     DDI1       161      0      1    9\n",
      "15     DDI1       133     DDI1       161      1      0    9\n",
      "16     DDI1       133     DDI1       190      0      1   10\n",
      "17     DDI1       133     DDI1       190      1      0   10\n",
      "18     DDI1       133     DDI1       213      0      1   11\n",
      "19     DDI1       133     DDI1       213      1      0   11\n",
      "20     DDI1       161     DDI1       161      0      1   12\n",
      "21     DDI1       161     DDI1       213      0      1   13\n",
      "22     DDI1       161     DDI1       213      1      0   13\n",
      "23     DDI1       213     DDI1       213      0      1   14\n",
      "24     DDI2        67     DDI2        77      0      1   15\n",
      "25     DDI2        67     DDI2        77      1      0   15\n",
      "26     DDI2        77     DDI2       153      0      1   16\n",
      "27     DDI2        77     DDI2       153      1      0   16\n",
      "28     DDI1       213     DDI2        77      0      0   17\n"
     ]
    }
   ],
   "source": [
    "# Process the dataframe further to create IMP input files for modeling\n",
    "\n",
    "# read in the csv file\n",
    "df_imp_process = pd.read_csv(os.path.join(xls_data_dir, 'final_bifunctional_data.csv'))\n",
    "print(f\"data dimensions for IMP processing: {df_imp_process.shape}\")\n",
    "\n",
    "# We are going to add three more columns to this dataframe: Copy1,Copy2,Iid\n",
    "# For each row, if Protein1 == Protein2 and Residue1 == Residue2, then the crosslink \n",
    "# has to be between two copies of the same protein, then Copy1 and Copy2 will be 0 and 1,\n",
    "# and Iid will be 1. Now, when we have Protein1 = Protein2 and Residue1 != Residue2, then\n",
    "# Copy1 and Copy2 could be 0 and 1, or 1 and 0 so two new rows will be created, and Iid\n",
    "# will be same number for both these rows which is continuously incremented for each new\n",
    "# entry created starting from 1. case 3 is when Protein1 != Protein2, in this case, \n",
    "# Copy1 and Copy2 could be 0 and 0 or 1 and 1 or 0 and 1 or 1 and 0, so Copy1 and Copy2 will be\n",
    "# 0 and 0, and Iid will be continuously incremented for each new entry\n",
    "\n",
    "rows = []\n",
    "iid = 1\n",
    "\n",
    "for row in df_imp_process.to_dict(orient='records'):\n",
    "    if row['Protein1'] == row['Protein2']:\n",
    "        if row['Residue1'] == row['Residue2']:\n",
    "            # Same protein and same residue:\n",
    "            row.update({'Copy1': 0, 'Copy2': 1, 'Iid': iid})\n",
    "            rows.append(row)\n",
    "            iid += 1\n",
    "        else:\n",
    "            # Same protein but different residues, create two rows with complementary Copy assignments\n",
    "            row1 = row.copy()\n",
    "            row1.update({'Copy1': 0, 'Copy2': 1, 'Iid': iid})\n",
    "            row2 = row.copy()\n",
    "            row2.update({'Copy1': 1, 'Copy2': 0, 'Iid': iid})\n",
    "            rows.extend([row1, row2])\n",
    "            iid += 1\n",
    "    else:\n",
    "        # Different proteins:\n",
    "        row.update({'Copy1': 0, 'Copy2': 0, 'Iid': iid})\n",
    "        rows.append(row)\n",
    "        iid += 1\n",
    "\n",
    "df_imp_process = pd.DataFrame(rows)\n",
    "print(df_imp_process)\n",
    "\n",
    "# save to file\n",
    "imp_file = os.path.join(xls_data_dir, 'ddi_bifunctional_imp_processed.csv')\n",
    "df_imp_process.to_csv(imp_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4642e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imptorch-latest-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
